# -*- coding: utf-8 -*-
"""multilabel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WXvx_awO2WaszPU0_q0ictYnVBoSw9LU
"""

from google.colab import drive
drive.mount('/content/drive')
#/content/drive/MyDrive/iproject/filtered.csv

cp /content/drive/MyDrive/iproject/wikifeature.zip .

cp /content/drive/MyDrive/iproject/index.zip .

!unzip index.zip indexed.csv

!unzip wikifeature.zip wikifeature.csv

from tensorflow.keras import layers
from tensorflow import keras
import tensorflow as tf
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import gc
import tensorflow as tf
import pandas as pd
import numpy as np
import re, string
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
import io
import json
import dask.dataframe as dd
import csv
import ast

from sklearn.model_selection import train_test_split
from ast import literal_eval

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import csv

del wiki_feature

wiki_feature = pd.read_csv('wikifeature.csv', names=['wikiid', 'wikilabel'],error_bad_lines=False,quoting=csv.QUOTE_NONE,)

print(f"There are {len(wiki_feature)} rows in the dataset.")

wiki_feature.head()

total_duplicate_titles = sum(wiki_feature["wikiid"].duplicated())
print(f"There are {total_duplicate_titles} duplicate ids.")

wiki_data_filtered = wiki_feature.groupby("wikilabel").filter(lambda x: len(x) > 1)
del wiki_feature
wiki_data_filtered.shape

del wiki_data_filtered

"""**Convert the string labels to lists of strings**"""

xpdf = pd.DataFrame(wiki_feature["wikilabel"][:10])
xpdf

gc.collect()

gc.enable()

del chunks

chunks = pd.read_csv('wikifeature.csv', names=['wikiid', 'wikilabel'],error_bad_lines=False,quoting=csv.QUOTE_NONE,chunksize= 20000000)
i = 0
while(True):
    try:
        wiki_feature = next(chunks)
        if wiki_feature.shape[0] < 1:
            break;
        wiki_data_filtered = wiki_feature.groupby("wikilabel").filter(lambda x: len(x) > 1)
        del wiki_feature
        wiki_data_filtered["wikilabel"] = wiki_data_filtered["wikilabel"].apply(lambda x: (str(x).lstrip(" ").split(" ")))
        i +=1
        print(i)
        if(i == 1):
          break
        #gc.collect()
    except Exception as e:
        print("error")

wiki_data_filtered.head()

chunks = pd.read_csv('indexed.csv', names=['wikiid', 'text'],error_bad_lines=False,quoting=csv.QUOTE_NONE,chunksize= 500000)
i = 0

while(True):
    try:
        indexed = next(chunks)
        if indexed.shape[0] < 1:
            break;
        subset = pd.merge(indexed, wiki_data_filtered, how='inner', on='wikiid')
        del wiki_data_filtered
        del indexed
        
        i +=1
        print(i)
        if(i == 1):
          break
        #gc.collect()
    except Exception as e:
        print("error")

"""**Multi-label binarization**"""

wikilabel = tf.ragged.constant(train_df["wikilabel"].values)
lookup = tf.keras.layers.StringLookup(output_mode="multi_hot")
lookup.adapt(wikilabel)
vocab = lookup.get_vocabulary()


def invert_multi_hot(encoded_labels):
    """Reverse a single multi-hot encoded label to a tuple of vocab terms."""
    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]
    return np.take(vocab, hot_indices)


print("Vocabulary:\n")
print(vocab)

deduplicated = subset[~subset["text"].duplicated()]

deduplicated.head()

deduplicated.to_csv('model_input.csv', mode='a', index=False, header=False)

!zip model_input.zip model_input.csv

!sudo apt install libtcmalloc-minimal4

!export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4

#cp model_input.zip /content/drive/MyDrive/iproject/

!cp /content/drive/MyDrive/iproject/model_input.zip .

!unzip model_input.zip model_input.csv

model_input_chk = pd.read_csv('model_input.csv', names=['wikiid', 'text', 'wikilabel'],error_bad_lines=False, chunksize= 100000)

model_input = next(model_input_chk)
model_input["wikilabel"] = model_input["wikilabel"].apply(
    lambda x: ast.literal_eval(x)
)

model_input.head()

def process_vector(line):
    line = str(line)
    linef = ""
    line = re.sub(r"[^A-Za-z0-9]"," ", line)
    #line = re.sub(r"[^p{L}]"," ", line)
    line = re.sub(r"^\W*","", line)
    line = re.sub(r"\W+"," ", line)
    line = re.sub(r"$\W*","", line)
    splitted = str(line).split(" ")[:10]
    line = " ".join(splitted)
    return line
    #return num_list

model_input["text"] = model_input["text"].apply(
    lambda x: process_vector(x)
)

model_input.head()

"""**Use stratified splits because of class imbalance**"""

test_split = 0.33

# Initial train and test split.
train_df, test_df = train_test_split(
    model_input,
    test_size=test_split
)

# Splitting the test set further into validation
# and new test sets.
val_df = test_df.sample(frac=0.5)
test_df.drop(val_df.index, inplace=True)

print(f"Number of rows in training set: {len(train_df)}")
print(f"Number of rows in validation set: {len(val_df)}")
print(f"Number of rows in test set: {len(test_df)}")

wikilabel = tf.ragged.constant(train_df["wikilabel"].values)
lookup = tf.keras.layers.StringLookup(output_mode="multi_hot")
lookup.adapt(wikilabel)
vocab = lookup.get_vocabulary()


def invert_multi_hot(encoded_labels):
    """Reverse a single multi-hot encoded label to a tuple of vocab terms."""
    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]
    return np.take(vocab, hot_indices)


print("Vocabulary:\n")
print(vocab)
print(len(vocab))

sample_label = train_df["wikilabel"].iloc[0]
print(f"Original label: {sample_label}")

label_binarized = lookup([sample_label])
print(f"Label-binarized representation: {label_binarized}")

train_df["text"].values

max_seqlen = 15
batch_size = 128
padding_token = "<pad>"
auto = tf.data.AUTOTUNE

def split_numbers(text):
  vector_list = []
  for num_str in text.values:
    vector = [int(num) for num in num_str.split(" ")][:9]
    #print(len(vector))
    vector_list.append(tf.convert_to_tensor(vector))
  return vector_list



def make_dataset(dataframe, is_train=True):
    labels = tf.ragged.constant(dataframe["wikilabel"].values)
    label_binarized = lookup(labels).numpy()
    dataset = tf.data.Dataset.from_tensor_slices(
        ((split_numbers(dataframe["text"])), label_binarized)
    )
    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset
    return dataset.batch(batch_size)



vocabulary = set()
train_df["text"].str.lower().str.split().apply(vocabulary.update)
vocabulary_size = len(vocabulary)
print(vocabulary_size)

train_dataset = make_dataset(train_df, is_train=True)
validation_dataset = make_dataset(val_df, is_train=False)
test_dataset = make_dataset(test_df, is_train=False)

text_vectorizer = layers.TextVectorization(
    max_tokens=vocabulary_size, ngrams=1, output_mode="tf_idf"
)

def process_tensorvector(line):
  string_v = tf.strings.as_string(line)
  return tf.convert_to_tensor()
    #return text_vectorizer(line)

# `TextVectorization` layer needs to be adapted as per the vocabulary from our
# training set.

train_dataset = train_dataset.map(
    lambda text, label: (text, label), num_parallel_calls=auto
).prefetch(auto)
validation_dataset = validation_dataset.map(
    lambda text, label: (text, label), num_parallel_calls=auto
).prefetch(auto)
test_dataset = test_dataset.map(
    lambda text, label: (text, label), num_parallel_calls=auto
).prefetch(auto)

text_batch, label_batch = next(iter(train_dataset))

for i, text in enumerate(text_batch[:5]):
    label = label_batch[i].numpy()[None, ...]
    print(type(text))
    print(tf.shape(text))
    print(f"Abstract: {text}")
    print(f"Label(s): {invert_multi_hot(label[0])}")
    print(" ")

def make_model():
    shallow_mlp_model = keras.Sequential(
        [
            layers.Dense(512, activation="relu"),
            layers.Dense(256, activation="relu"),
            layers.Dense(lookup.vocabulary_size(), activation="sigmoid"),
        ]  # More on why "sigmoid" has been used here in a moment.
    )
    return shallow_mlp_model

epochs = 20

shallow_mlp_model = make_model()
shallow_mlp_model.compile(
    loss="binary_crossentropy", optimizer="adam", metrics=["binary_accuracy"]
)

history = shallow_mlp_model.fit(
    train_dataset, validation_data=validation_dataset, epochs=epochs
)


def plot_result(item):
    plt.plot(history.history[item], label=item)
    plt.plot(history.history["val_" + item], label="val_" + item)
    plt.xlabel("Epochs")
    plt.ylabel(item)
    plt.title("Train and Validation {} Over Epochs".format(item), fontsize=14)
    plt.legend()
    plt.grid()
    plt.show()


plot_result("loss")
plot_result("binary_accuracy")

"""**The below is the data preparation code**"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import gc
import tensorflow as tf
import pandas as pd
import numpy as np
import re, string
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
import io
import json
import dask.dataframe as dd
import csv

import logging
logger = logging.getLogger('wikitokeniser')
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
hdlr = logging.FileHandler('tokenizer.log')
hdlr.setFormatter(formatter)
logger.addHandler(hdlr)
logger.setLevel(logging.INFO)
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
nltk.download('wordnet')
nltk.download('omw-1.4')
lemma = WordNetLemmatizer()
from scipy.sparse import csr_matrix

gc.collect()

chunks = pd.read_csv('traindata.out', delimiter='\t', 
                     chunksize= 500000, header=None,
                     names=['wikiid', 'token', 'label'],error_bad_lines=False)

def filter_stopwords(line):
    linef = ""
    splitted = str(line).split(" ")
    for w in splitted:
        if w not in stop_words:
            linef = linef+" "+w
    return linef

def process_sentence(line):
    line = str(line)
    linef = ""
    line = re.sub(r"[^A-Za-z0-9]"," ", line)
    #line = re.sub(r"[^p{L}]"," ", line)
    line = re.sub(r"^\W*","", line)
    line = re.sub(r"$\W*","", line)
    line = re.sub(r"\W+"," ", line)
    splitted = str(line).split(" ")
    for w in splitted:
        if w not in stop_words:
            word_lemma = lemma.lemmatize(w)
            linef = linef+" "+word_lemma
    return linef

def isWikiId1(line):
    line = re.sub(r"Q[0-9]+","",str(line))
    if len(line) == 0:
        return "1"
    else:
        return "0"

def isWikiId(line):
    line = re.sub(r"Q[0-9]+","",str(line))
    if len(line) == 0:
        return "1"
    else:
        line = re.sub(r"^\W*","", line)
        line = re.sub(r"(nan)","", line)
        if len(line) == 0:
            return "2"
    return "0"

#gc.enable()
while(True):
    try:
        df = next(chunks)
        if df.shape[0] < 1:
            break;
        #df['rank'] = df.groupby('wikiid')['wikiid'].rank('first')
        df['isWiki'] = df.token.map(lambda x : isWikiId(x))
        red_df = df[df['isWiki'] == "0"]
        red_df['filtered_token'] = red_df.token.map(lambda x : process_sentence(x))
        new_df = red_df[['wikiid', 'filtered_token']]
        wiki_df = df[df['isWiki'] == "1"]
        wiki_df = wiki_df[['wikiid', 'token']]
        new_df.to_csv('filtered_1.csv', mode='a', index=False, header=False)
        wiki_df.to_csv('wikirelation_1.csv', mode='a', index=False, header=False)
        #gc.collect()
    except Exception as e:
        logger.error('Failed to parse this block: '+ str(e))
        continue

df2 = pd.read_csv('filtered.csv',header=None,names=['wikiid', 'token'], chunksize= 1000000,error_bad_lines=False)
desired_vocab_size = 20000000
t = tf.keras.preprocessing.text.Tokenizer(num_words=desired_vocab_size)
gc.enable()

i = 0
while(True):
    i += 1
    
    try:
        wikidf = next(df2)
        if wikidf.shape[0] < 1:
            break;
        wikidf['wikiid'] = wikidf['wikiid'].astype(str)
        wikidf['token'] = wikidf['token'].astype(str)
        t.fit_on_texts(wikidf['token'])
        print(i)
    except:
        print(len(t.word_index))

tok_to_json = t.to_json()

with io.open('wikitoken.json', 'r') as f:
  json_token = json.load(f)
  t = tf.keras.preprocessing.text.tokenizer_from_json(json_token)

with io.open('wikitoken.json', 'w', encoding='utf-8') as f:
  f.write(json.dumps(tok_to_json, ensure_ascii=False))

X_train, X_test, y_train, y_test = train_test_split(wikidf['token'], wikidf['wikiid'], test_size = 0.3, random_state = 42)

!cp /content/wikitoken.json /content/wikitoken.json /content/drive/MyDrive/iproject

/content/indexed.csv

chunks = pd.read_csv('filtered.csv',  
                     chunksize= 500000, header=None,
                     names=['wikiid', 'text'],error_bad_lines=False)

max_length = 15
i = 0
while(True):
    try:
        indexed_df = next(chunks)
        indexed_df.dropna(inplace=True)
        if indexed_df.shape[0] < 1:
            break;
        indexed_df['wikiid'] = indexed_df['wikiid'].astype(str)
        indexed_df['text'] = indexed_df['text'].astype(str)
        indexed_ = t.texts_to_sequences(indexed_df.text.to_list())
        padded_ = tf.keras.preprocessing.sequence.pad_sequences(indexed_,
                                                                      maxlen = max_length,
                                                                      padding = 'post',
                                                                      truncating = 'post')
        indexed_df['padded_token'] = list(padded_)
        indexed_df[['wikiid','padded_token']].to_csv('indexed.csv', mode='a', index=False, header=False)
       
        i +=1
        if (i%50 == 0):
          print(i)
        #gc.collect()
    except Exception as e:
        print(i)
        break
        logger.error('Failed to parse this block: '+ str(e))

!zip index.zip indexed.csv
!cp index.zip /content/drive/MyDrive/iproject/

!cp index.zip /content/drive/MyDrive/iproject/



from google.colab import drive
drive.mount('/content/drive')

cd /content

!zip 5g index.zip indexed.csv

!cp /content/drive/MyDrive/iproject/input.zip .

!unzip input.zip

"""# New Section"""

!pip install dask

import dask.dataframe as dd
import csv

wiki_indexed_chk = pd.read_csv("indexed.csv",names=['wikiid', 'wiki_ind_sent'],error_bad_lines=False, quoting=csv.QUOTE_NONE,chunksize= 1000000)



wiki_feature.head(5)

from sklearn.feature_extraction.text import TfidfVectorizer

wiki_feature = pd.read_csv('wikifeature.csv', names=['wikiid', 'wikilabel'],error_bad_lines=False,quoting=csv.QUOTE_NONE,)

desired_label_size = 100
tfidf = TfidfVectorizer(max_features=desired_label_size)

tfidf.fit(wiki_feature['wikilabel'])

tfidf







chunks = pd.read_csv('wikifeature.csv', names=['wikiid', 'wikilabel'],error_bad_lines=False,quoting=csv.QUOTE_NONE,chunksize= 1024 )


max_length = 15
i = 0
while(True):
    try:
        label_df = next(chunks)
        if label_df.shape[0] < 1:
            break;
        wiki_feature['label'] = tfidf.transform(wiki_feature['wikilabel'])
        wiki_feature['mapped'] = csr_matrix.sum(wiki_feature.label)

        wiki_filtered_label = wiki_feature[wiki_feature['mapped'] > 0]
       
        i +=1

        wiki_filtered_label[['wikiid','label']].to_csv('wiki_filtered_label.csv', mode='a', index=False, header=False)
        if (i == 1):
          break
          #print(i)
    except Exception as e:
        print(i)
        break
        logger.error('Failed to parse this block: '+ str(e))

tfidf.transform(wiki_feature['wikilabel'])

wiki_filtered_label

desired_label_size = 512
#tx = tf.keras.preprocessing.text.Tokenizer(num_words=desired_label_size)

tx.fit_on_texts(dfx['wikilabel'])

x = tx.texts_to_sequences(dfx.wikilabel.to_list())
p = tf.keras.preprocessing.sequence.pad_sequences(x,
                                                              maxlen = 15,
                                                              padding = 'post',
                                                              truncating = 'post')



#

tfidf.fit(dfx['wikilabel'])

x = tfidf.transform(dfx['wikilabel'])

x.todense().shape

while(True):
    try:
        wiki_indexed = next(wiki_indexed_chk)
        if wiki_indexed.shape[0] < 1:
            break;
        #df['rank'] = df.groupby('wikiid')['wikiid'].rank('first')
        wiki_df = pd.merge(wiki_indexed,wiki_feature, on="wikiid", how='left')
  
        wiki_df.to_csv('wikiindexedwlabel.csv', mode='a', index=False, header=False)
        #gc.collect()
    except Exception as e:
        logger.error('Failed to parse this block: '+ str(e))
        continue



"""# New Section"""