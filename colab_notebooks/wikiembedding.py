# -*- coding: utf-8 -*-
"""wikiembedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WXvx_awO2WaszPU0_q0ictYnVBoSw9LU
"""

from google.colab import drive
drive.mount('/content/drive')
#/content/drive/MyDrive/iproject/filtered.csv

cp /content/drive/MyDrive/iproject/index.zip .

cp /content/drive/MyDrive/iproject/wikifeature.zip .

!unzip wikifeature.zip wikifeature.csv

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import gc
import tensorflow as tf
import pandas as pd
import numpy as np
import re, string
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
import io
import json
import dask.dataframe as dd
import csv

import logging
logger = logging.getLogger('wikitokeniser')
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
hdlr = logging.FileHandler('tokenizer.log')
hdlr.setFormatter(formatter)
logger.addHandler(hdlr)
logger.setLevel(logging.INFO)
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
nltk.download('wordnet')
nltk.download('omw-1.4')
lemma = WordNetLemmatizer()
from scipy.sparse import csr_matrix



chunks = pd.read_csv('traindata.out', delimiter='\t', 
                     chunksize= 500000, header=None,
                     names=['wikiid', 'token', 'label'],error_bad_lines=False)

def filter_stopwords(line):
    linef = ""
    splitted = str(line).split(" ")
    for w in splitted:
        if w not in stop_words:
            linef = linef+" "+w
    return linef

def process_sentence(line):
    line = str(line)
    linef = ""
    line = re.sub(r"[^A-Za-z0-9]"," ", line)
    #line = re.sub(r"[^p{L}]"," ", line)
    line = re.sub(r"^\W*","", line)
    line = re.sub(r"$\W*","", line)
    line = re.sub(r"\W+"," ", line)
    splitted = str(line).split(" ")
    for w in splitted:
        if w not in stop_words:
            word_lemma = lemma.lemmatize(w)
            linef = linef+" "+word_lemma
    return linef

def isWikiId1(line):
    line = re.sub(r"Q[0-9]+","",str(line))
    if len(line) == 0:
        return "1"
    else:
        return "0"

def isWikiId(line):
    line = re.sub(r"Q[0-9]+","",str(line))
    if len(line) == 0:
        return "1"
    else:
        line = re.sub(r"^\W*","", line)
        line = re.sub(r"(nan)","", line)
        if len(line) == 0:
            return "2"
    return "0"

#gc.enable()
while(True):
    try:
        df = next(chunks)
        if df.shape[0] < 1:
            break;
        #df['rank'] = df.groupby('wikiid')['wikiid'].rank('first')
        df['isWiki'] = df.token.map(lambda x : isWikiId(x))
        red_df = df[df['isWiki'] == "0"]
        red_df['filtered_token'] = red_df.token.map(lambda x : process_sentence(x))
        new_df = red_df[['wikiid', 'filtered_token']]
        wiki_df = df[df['isWiki'] == "1"]
        wiki_df = wiki_df[['wikiid', 'token']]
        new_df.to_csv('filtered_1.csv', mode='a', index=False, header=False)
        wiki_df.to_csv('wikirelation_1.csv', mode='a', index=False, header=False)
        #gc.collect()
    except Exception as e:
        logger.error('Failed to parse this block: '+ str(e))
        continue

df2 = pd.read_csv('filtered.csv',header=None,names=['wikiid', 'token'], chunksize= 1000000,error_bad_lines=False)
desired_vocab_size = 20000000
t = tf.keras.preprocessing.text.Tokenizer(num_words=desired_vocab_size)
gc.enable()

i = 0
while(True):
    i += 1
    
    try:
        wikidf = next(df2)
        if wikidf.shape[0] < 1:
            break;
        wikidf['wikiid'] = wikidf['wikiid'].astype(str)
        wikidf['token'] = wikidf['token'].astype(str)
        t.fit_on_texts(wikidf['token'])
        print(i)
    except:
        print(len(t.word_index))

tok_to_json = t.to_json()

with io.open('wikitoken.json', 'r') as f:
  json_token = json.load(f)
  t = tf.keras.preprocessing.text.tokenizer_from_json(json_token)

with io.open('wikitoken.json', 'w', encoding='utf-8') as f:
  f.write(json.dumps(tok_to_json, ensure_ascii=False))

X_train, X_test, y_train, y_test = train_test_split(wikidf['token'], wikidf['wikiid'], test_size = 0.3, random_state = 42)

!cp /content/wikitoken.json /content/wikitoken.json /content/drive/MyDrive/iproject

/content/indexed.csv

chunks = pd.read_csv('filtered.csv',  
                     chunksize= 500000, header=None,
                     names=['wikiid', 'text'],error_bad_lines=False)

max_length = 15
i = 0
while(True):
    try:
        indexed_df = next(chunks)
        indexed_df.dropna(inplace=True)
        if indexed_df.shape[0] < 1:
            break;
        indexed_df['wikiid'] = indexed_df['wikiid'].astype(str)
        indexed_df['text'] = indexed_df['text'].astype(str)
        indexed_ = t.texts_to_sequences(indexed_df.text.to_list())
        padded_ = tf.keras.preprocessing.sequence.pad_sequences(indexed_,
                                                                      maxlen = max_length,
                                                                      padding = 'post',
                                                                      truncating = 'post')
        indexed_df['padded_token'] = list(padded_)
        indexed_df[['wikiid','padded_token']].to_csv('indexed.csv', mode='a', index=False, header=False)
       
        i +=1
        if (i%50 == 0):
          print(i)
        #gc.collect()
    except Exception as e:
        print(i)
        break
        logger.error('Failed to parse this block: '+ str(e))

!zip index.zip indexed.csv
!cp index.zip /content/drive/MyDrive/iproject/

!cp index.zip /content/drive/MyDrive/iproject/



from google.colab import drive
drive.mount('/content/drive')

cd /content

!zip 5g index.zip indexed.csv

!cp /content/drive/MyDrive/iproject/input.zip .

!unzip input.zip

"""# New Section"""

!pip install dask

import dask.dataframe as dd
import csv

wiki_indexed_chk = pd.read_csv("indexed.csv",names=['wikiid', 'wiki_ind_sent'],error_bad_lines=False, quoting=csv.QUOTE_NONE,chunksize= 1000000)



wiki_feature.head(5)

from sklearn.feature_extraction.text import TfidfVectorizer

wiki_feature = pd.read_csv('wikifeature.csv', names=['wikiid', 'wikilabel'],error_bad_lines=False,quoting=csv.QUOTE_NONE,)

desired_label_size = 100
tfidf = TfidfVectorizer(max_features=desired_label_size)

tfidf.fit(wiki_feature['wikilabel'])

tfidf







chunks = pd.read_csv('wikifeature.csv', names=['wikiid', 'wikilabel'],error_bad_lines=False,quoting=csv.QUOTE_NONE,chunksize= 1024 )


max_length = 15
i = 0
while(True):
    try:
        label_df = next(chunks)
        if label_df.shape[0] < 1:
            break;
        wiki_feature['label'] = tfidf.transform(wiki_feature['wikilabel'])
        wiki_feature['mapped'] = csr_matrix.sum(wiki_feature.label)

        wiki_filtered_label = wiki_feature[wiki_feature['mapped'] > 0]
       
        i +=1

        wiki_filtered_label[['wikiid','label']].to_csv('wiki_filtered_label.csv', mode='a', index=False, header=False)
        if (i == 1):
          break
          #print(i)
    except Exception as e:
        print(i)
        break
        logger.error('Failed to parse this block: '+ str(e))

tfidf.transform(wiki_feature['wikilabel'])

wiki_filtered_label

desired_label_size = 512
#tx = tf.keras.preprocessing.text.Tokenizer(num_words=desired_label_size)

tx.fit_on_texts(dfx['wikilabel'])

x = tx.texts_to_sequences(dfx.wikilabel.to_list())
p = tf.keras.preprocessing.sequence.pad_sequences(x,
                                                              maxlen = 15,
                                                              padding = 'post',
                                                              truncating = 'post')



#

tfidf.fit(dfx['wikilabel'])

x = tfidf.transform(dfx['wikilabel'])

x.todense().shape

while(True):
    try:
        wiki_indexed = next(wiki_indexed_chk)
        if wiki_indexed.shape[0] < 1:
            break;
        #df['rank'] = df.groupby('wikiid')['wikiid'].rank('first')
        wiki_df = pd.merge(wiki_indexed,wiki_feature, on="wikiid", how='left')
  
        wiki_df.to_csv('wikiindexedwlabel.csv', mode='a', index=False, header=False)
        #gc.collect()
    except Exception as e:
        logger.error('Failed to parse this block: '+ str(e))
        continue



"""# New Section"""